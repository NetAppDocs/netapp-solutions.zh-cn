---
sidebar: sidebar 
permalink: data-analytics/apache-spark-solution-technology.html 
keywords: standalone, apache mesos, hadoop yarn, resilient distributed dataset, rdd, dataframe, hadoop distributed file system, hdfs 
summary: 本节介绍Apache Spark的性质和组件、以及它们对解决方案 的贡献。 
---
= 解决方案技术
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./../media/


[role="lead"]
Apache Spark是一种常见的编程框架、用于编写直接与Hadoop分布式文件系统(HDFS)配合使用的Hadoop应用程序。SPARK可随时投入生产、支持流式数据处理、并且速度比MapReduce更快。Spark具有可配置的内存数据缓存功能、可实现高效迭代、而Spark shell可通过交互方式学习和探索数据。借助Spark、您可以在Python、Scala或Java中创建应用程序。激发应用程序由一个或多个具有一个或多个任务的作业组成。

每个Spark应用程序都有一个Spark驱动程序。在yar-Client模式下、驱动程序在本地客户端上运行。在yar-Cluster模式下、驱动程序在应用程序主节点上的集群中运行。在集群模式下、即使客户端断开连接、应用程序也会继续运行。

image::apache-spark-image3.png[Apache Spark image3]

集群管理器有三种：

* *独立。*此管理器是Spark的一部分、可轻松设置集群。
* * Apache Mesos.*这是一个通用集群管理器、也运行MapReduce和其他应用程序。
* * Hadoop yaryar.*这是Hadoop 3中的资源管理器。


弹性分布式数据集(RDD)是Spark的主要组件。RDD会重新创建存储在集群内存中的数据中的丢失和缺失数据、并存储来自文件或通过编程方式创建的初始数据。可以使用文件、内存中的数据或其他RDD创建RDD。SPARK编程执行两项操作：转型和操作。转型将基于现有RDD创建新的RDD。操作将从RDD返回一个值。

转换和操作也适用于Spark数据集和DataFrame。数据集是一组分布式数据、可提供RDD的优势(强类型、使用lambda函数)以及Spark SQL优化的执行引擎的优势。可以使用JVM对象构建数据集、然后使用功能转换(map、flatMap、filter等)进行操作。DataFrame是按命名列组织的数据集。它在概念上相当于关系数据库中的表或R/Python中的数据帧。DataFrame可以从多种源构建、例如结构化数据文件、Hive/HBase中的表、内部或云中的外部数据库或现有RDD。

Spark应用程序包括一个或多个Spark作业。作业在执行器中运行任务、而执行器在YARN容器中运行。每个执行者在一个容器中运行、执行者在应用程序的整个生命周期内都存在。执行者在应用程序启动后得到修复、而yarn不会调整已分配的容器的大小。执行者可以同时对内存数据运行任务。
