---
sidebar: sidebar 
permalink: ai/aipod_nv_storage.html 
keywords: NetApp AI, AI, Artificial Intelligence, ML, Machine Learning, NVIDIA, NVIDIA AI Enterprise, NVIDIA BasePOD, NVIDIA DGX 
summary: 采用NVIDIA DGX系统的NetApp AI Pod—存储系统设计和大小规划指南 
---
= 采用NVIDIA DGX系统的NetApp AI Pod—存储系统设计和大小规划指南
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./../media/


link:aipod_nv_architecture.html["上一页：采用NVIDIA DGX系统的NetApp AI Pod—架构。"]



== 存储系统设计

每个AFF A800存储系统使用每个控制器中的四个100 GbE端口进行连接。每个控制器的两个端口用于从DGX系统访问工作负载数据、每个控制器的两个端口配置为LACP接口组、以支持从管理平台服务器访问集群管理项目和用户主目录。存储系统的所有数据访问均通过NFS提供、其中一个Storage Virtual Machine (SVM)专用于AI工作负载访问、另一个SVM专用于集群管理用途。

工作负载SVM总共配置了四个逻辑接口(Logical Interface、Logical Interface、Logical Interface、简称为Logical Interface、简称为Logical Interface、简称为Logical Interface、简称为Logical Interface、简称为Logical Interface、简称为Logical Interface、简称每个物理端口托管一个两个LIP、因此每个控制器上的每个VLAN具有两个LIP。此配置可提供最大带宽、并允许每个LIF故障转移到同一控制器上的另一个端口、以便在发生网络故障时两个控制器保持活动状态。此配置还支持基于RDMA的NFS以启用GPUDirect存储访问。存储容量以一个大型FlexGroup卷的形式配置、该卷跨越两个控制器。此可从SVM上的任何FlexGroup进行访问、DGX A100系统的挂载点会分布在可用的LUN之间、以实现负载平衡。

管理SVM仅需要一个LIF、该LIF托管在每个控制器上配置的双端口接口组上。在管理SVM上配置了其他FlexGroup卷、用于存放集群管理项目、例如集群节点映像、系统监控历史数据和最终用户主目录。下图显示了存储系统的逻辑配置。

image:oai_basepod1_logical.png["错误：缺少图形映像"]



== 存储系统大小指导

此架构可供希望使用NVIDIA DGX系统和NetApp AFF存储系统实施深度学习基础架构的客户和合作伙伴参考。下表显示了每个AFF型号所支持的A100和H100 GPU数量的粗略估计。

image:oai_sizing.png["错误：缺少图形映像"]

如中所示 link:https://www.netapp.com/pdf.html?item=/media/21793-nva-1153-design.pdf["此参考架构的先前版本"]AFF A800系统可轻松支持八个DGX A100系统生成的深度学习训练工作负载。上述其他存储系统的估计值是根据这些结果计算的、而H100 GPU的估计值是通过将A100系统所需的存储吞吐量增加一倍来计算的。  对于存储性能要求较高的大型部署、可以在一个集群中向NetApp ONTAP集群添加更多AFF系统、最多可添加12个HA对(24个节点)。使用本解决方案中所述的FlexGroup技术、一个24节点集群可以在一个命名空间中提供超过40 PB的吞吐量和高达300 Gbps的吞吐量。其他NetApp存储系统(例如AFF A400、A250和C800)以更低的成本为小型部署提供了更低的性能和/或更高的容量选项。由于ONTAP 9支持混合模式集群、因此客户可以先减少初始占用空间、然后随着容量和性能要求的增长向集群添加更多或更大的存储系统。
link:aipod_nv_conclusion.html["下一步：采用NVIDIA DGX系统的NetApp AI Pod—总结。"]
