---
sidebar: sidebar 
permalink: ai/ai-edge-test-procedure.html 
keywords: procedure, operating system, ubuntu, nvidia, docker, criteo, brats 
summary: 本节介绍用于验证此解决方案的测试过程。 
---
= 测试操作步骤
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./../media/


[role="lead"]
本节介绍用于验证此解决方案的测试过程。



== 操作系统和 AI 推理设置

对于 AFF C190 ，我们使用支持 NVIDIA GPU 的 Ubuntu 18.04 和 Docker ，并使用 MLPerf https://github.com/mlperf/inference_results_v0.7/tree/master/closed/Lenovo["code"^] 在联想提交到 MLPerf 推理 v0.7 的过程中提供。

对于 EF280 ，我们将 Ubuntu 20.04 与 NVIDIA 驱动程序结合使用，并将 Docker 与 NVIDIA GPU 和 MLPerf 结合使用 https://github.com/mlcommons/inference_results_v1.1/tree/main/closed/Lenovo["code"^] 在联想提交到 MLPerf 推理 v1.1 的过程中提供。

要设置 AI 推理，请执行以下步骤：

. 下载需要注册的数据集， ImageNet 2012 验证集， Criteo TB 数据集和 Brats 2019 训练集，然后解压缩这些文件。
. 创建一个至少包含 1 TB 的工作目录，并定义环境变量 `MLPERF_scrating_path` 引用该目录。
+
在使用本地数据进行测试时，您应在共享存储上共享此目录以供网络存储使用情形使用，或者在本地磁盘上共享此目录。

. 运行 make `prebuild` 命令，该命令可为所需的推理任务构建并启动 Docker 容器。
+

NOTE: 以下命令全部从正在运行的 Docker 容器中执行：

+
** 为 MLPerf 推理任务下载经过预先培训的 AI 型号： `make download_model`
** 下载可免费下载的其他数据集： `make download_data`
** 预处理数据： make `preprocess_data`
** 运行： `m构建` 。
** 针对计算服务器中的 GPU 优化的构建推理引擎： `make generate_engine`
** 要运行推理工作负载，请运行以下命令（一个命令）：




....
make run_harness RUN_ARGS="--benchmarks=<BENCHMARKS> --scenarios=<SCENARIOS>"
....


== AI 推理运行

执行了三种类型的运行：

* 使用本地存储的单服务器 AI 推理
* 使用网络存储的单服务器 AI 推理
* 使用网络存储的多服务器 AI 推理

